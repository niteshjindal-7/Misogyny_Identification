{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d766c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 14:12:56.387218: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch \n",
    "from transformers import ViTImageProcessor\n",
    "from PIL import Image\n",
    "from transformers import ViTForImageClassification\n",
    "from torchvision.transforms import functional as F\n",
    "import natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14659f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of common strings 9986\n",
      "9986 10000 9986\n",
      "len of common strings 1000\n",
      "1000 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26186/160188111.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_txt_file['prefix_file_name'] = sub_txt_file['file_name'].str.extract('(\\d+)').astype(int)\n"
     ]
    }
   ],
   "source": [
    "img_IDs = []\n",
    "updated_pathnames = []\n",
    "dir_path = os.path.join(os.getcwd() , 'TRAINING')\n",
    "img_filenames= natsort.natsorted(os.listdir(dir_path))  \n",
    "\n",
    "# get the ids from the images, where images are having three channels; omit images if channels != 3\n",
    "for i, filename in enumerate(img_filenames):\n",
    "#     print(i, filename)\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        path_name=os.path.join(dir_path, filename)\n",
    "        im = Image.open(path_name)\n",
    "        im = im.resize((224, 224))  # Resize the image to (224, 224)\n",
    "        imnp = np.array(im)\n",
    "        if len(imnp.shape) != 3:\n",
    "            continue\n",
    "        img_IDs.append(filename)\n",
    "        updated_pathnames.append(path_name)  #gives 9986 results\n",
    "        \n",
    "\n",
    "def get_common_strings(list1, list2):\n",
    "    return list(set(list1) & set(list2))\n",
    "\n",
    "\n",
    "txt_file = pd.read_csv('./training1.csv', delimiter='\\t')\n",
    "sub_txt_file = txt_file[['file_name', 'misogynous']]\n",
    "sub_txt_file['prefix_file_name'] = sub_txt_file['file_name'].str.extract('(\\d+)').astype(int)\n",
    "sub_txt_file = sub_txt_file.sort_values(by='prefix_file_name', ascending=True)\n",
    "\n",
    "\n",
    "txt_IDs = list(sub_txt_file.file_name)  #from the text file where we have text description \n",
    "common_strings = get_common_strings(txt_IDs, img_IDs)\n",
    "print('len of common strings', len(common_strings))\n",
    "\n",
    "common_IDs = natsort.natsorted(common_strings)\n",
    "print(len(common_IDs), len(txt_IDs), len(img_IDs))\n",
    "\n",
    "\n",
    "filtered_txt_file = sub_txt_file[sub_txt_file['file_name'].isin(common_IDs)].reset_index(drop=True)\n",
    "filtered_txt_file\n",
    "\n",
    "train_labels= list(filtered_txt_file['misogynous'])\n",
    "\n",
    "\n",
    "train_images_path = []\n",
    "for i in common_IDs:\n",
    "    path_name=os.path.join(dir_path, i)\n",
    "    train_images_path.append(path_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "######################################## TEST IMAGES PATH ###################################\n",
    "# Get IDs from test text data\n",
    "test1 = pd.read_csv('Test.csv', delimiter='\\t')\n",
    "test_labels = pd.read_csv('test_labels.txt', \n",
    "                          delimiter='\\t',\n",
    "                         header=None)\n",
    "\n",
    "test_labels.columns = ['file_name', \n",
    "                      \"misogynous\",\n",
    "                       \"shaming\",\n",
    "                       \"stereotype\",\n",
    "                       \"objectification\",\n",
    "                       \"violence\"]\n",
    "\n",
    "merged_test = pd.merge(test1, test_labels, on='file_name', how='inner')\n",
    "\n",
    "# Sort the dataframe with natural ordering of the IDs\n",
    "merged_test['prefix_file_name'] = merged_test['file_name'].str.extract('(\\d+)').astype(int)\n",
    "# Assuming 'df' is your DataFrame\n",
    "merged_test1 = merged_test.sort_values(by='prefix_file_name', ascending=True)\n",
    "merged_test1\n",
    "\n",
    "# # train = train.rename(columns={'Text Transcription': 'text'})\n",
    "test2 = merged_test1.rename(columns={'Text Transcription': 'text'})\n",
    "test_txt_IDs = list(test2['file_name'])\n",
    "\n",
    "\n",
    "\n",
    "# GET IDs from test images data\n",
    "test_dir_path = os.path.join(os.getcwd() , 'TEST')\n",
    "test_img_IDs = natsort.natsorted(os.listdir(test_dir_path))  \n",
    "\n",
    "# Get COmmon IDS\n",
    "test_common_strings = get_common_strings(test_txt_IDs, test_img_IDs)\n",
    "test_common_IDs = natsort.natsorted(test_common_strings)\n",
    "print('len of common strings', len(test_common_IDs))\n",
    "\n",
    "\n",
    "test_images_path = []\n",
    "for i in test_common_IDs:\n",
    "    path_name=os.path.join(test_dir_path, i)\n",
    "    test_images_path.append(path_name)\n",
    "    \n",
    "# test_images_path\n",
    "tst_filtered_txt_file = test2[test2['file_name'].isin(test_common_IDs)].reset_index(drop=True)\n",
    "test_labels= list(tst_filtered_txt_file['misogynous'])\n",
    "# test_images_path[-1], test_labels[-1]\n",
    "\n",
    "print(len(test_images_path), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c5daebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total image info created\n",
      "Results saved as pickle object.\n",
      "total image info created\n",
      "Results saved as pickle object.\n",
      "total image info created\n",
      "Results saved as pickle object.\n"
     ]
    }
   ],
   "source": [
    "def img_info(images_path, labels, batch_size=1):\n",
    "    for i in range(0, len(images_path), batch_size):\n",
    "#         image_batch = []\n",
    "        for j in range(i, min(i+batch_size, len(images_path))):\n",
    "            image_path = images_path[j]\n",
    "            image = Image.open(image_path)\n",
    "            image_info = { 'image': image, 'image_file_path': image_path, 'labels': labels[j] }\n",
    "#             image_batch.append(image_info)\n",
    "\n",
    "        yield image_info\n",
    "    \n",
    "train_images_path1 = train_images_path[0:3000]\n",
    "train_labels1 = train_labels[0:3000]\n",
    "\n",
    "\n",
    "total_img_info1=[]\n",
    "for k in img_info(train_images_path1, train_labels1, batch_size=1):\n",
    "    total_img_info1.append(k)\n",
    "    \n",
    "\n",
    "print('total image info created')    \n",
    "\n",
    "import pickle\n",
    "with open(os.path.join('18_ViTEmbeddings', '18_total_img_info1.pickle'), 'wb') as f:\n",
    "    pickle.dump(total_img_info1, f)\n",
    "\n",
    "print(\"Results saved as pickle object.\")\n",
    "\n",
    "del total_img_info1,  train_images_path1 , train_labels1\n",
    "\n",
    "# ###########################################################\n",
    "\n",
    "\n",
    "train_images_path2 = train_images_path[3000:6000]\n",
    "train_labels2 = train_labels[3000:6000]\n",
    "\n",
    "\n",
    "total_img_info2=[]\n",
    "for k in img_info(train_images_path2, train_labels2, batch_size=1):\n",
    "    total_img_info2.append(k)\n",
    "    \n",
    "print('total image info created')    \n",
    "\n",
    "import pickle\n",
    "# Save the list as a pickle object\n",
    "with open(os.path.join('18_ViTEmbeddings', '18_total_img_info2.pickle'), 'wb') as f:\n",
    "    pickle.dump(total_img_info2, f)\n",
    "\n",
    "print(\"Results saved as pickle object.\")\n",
    "\n",
    "del total_img_info2, train_images_path2 , train_labels2 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ########################################################\n",
    "\n",
    "train_images_path3 = train_images_path[6000:len(train_images_path)]\n",
    "train_labels3 = train_labels[6000:len(train_images_path)]\n",
    "\n",
    "\n",
    "total_img_info3=[]\n",
    "for k in img_info(train_images_path3, train_labels3, batch_size=1):\n",
    "    total_img_info3.append(k)\n",
    "    \n",
    "print('total image info created')    \n",
    "\n",
    "import pickle\n",
    "# Save the list as a pickle object\n",
    "with open(os.path.join('18_ViTEmbeddings', '18_total_img_info3.pickle'), 'wb') as f:\n",
    "    pickle.dump(total_img_info3, f)\n",
    "\n",
    "print(\"Results saved as pickle object.\")\n",
    "\n",
    "del total_img_info3, train_images_path3 , train_labels3 \n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# List of pickle file names you want to load\n",
    "pickle_files = ['18_total_img_info1.pickle', '18_total_img_info2.pickle', '18_total_img_info3.pickle']\n",
    "# pickle_files = ['18_total_img_info1.pickle']\n",
    "\n",
    "# Create an empty list to store the combined results\n",
    "total_img_info = []\n",
    "\n",
    "# Assuming '18_ViTEmbeddings' is the directory containing your pickle files\n",
    "\n",
    "\n",
    "total_img_info = []\n",
    "for file_name in pickle_files:\n",
    "    fn = os.path.join('18_ViTEmbeddings', file_name)\n",
    "    with open(fn, 'rb') as f:\n",
    "        loaded_list = pickle.load(f)\n",
    "        total_img_info.extend(loaded_list)\n",
    "        \n",
    "for k in img_info(test_images_path, test_labels, batch_size=1):\n",
    "    total_img_info.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba5e8e",
   "metadata": {},
   "source": [
    "## Build Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb56343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nitesh/env/dev38/python38/lib/python3.8/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms.functional import resize\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "\n",
    "# Load the ViT feature extractor and model\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "# model = ViTForImageClassification.from_pretrained(model_name)\n",
    "model = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Helper function to process a batch of images and get mean-pooled embeddings\n",
    "def process_batch(batch, start_idx, end_idx):\n",
    "    mean_pooled_embeddings = []\n",
    "    lbls = []\n",
    "    for image_info in batch:\n",
    "        image = image_info['image']\n",
    "        lbl = image_info['labels']\n",
    "        image_file_path = image_info['image_file_path']\n",
    "\n",
    "        # Resize the image to the desired shape (224x224)\n",
    "        image = resize(image, (224, 224))\n",
    "\n",
    "        # Preprocess the image for the ViT model\n",
    "        inputs = feature_extractor(images=image, return_tensors=\"pt\")  # Make sure to return tensor\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Get the output embeddings (features)\n",
    "        feature_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "        # Apply mean pooling along the sequence length dimension\n",
    "        mean_pooled_embedding = torch.mean(feature_embeddings, dim=0)\n",
    "\n",
    "        # Append the mean-pooled embedding to the list\n",
    "        mean_pooled_embeddings.append(mean_pooled_embedding)\n",
    "        lbls.append(lbl)\n",
    "\n",
    "    # Stack the embeddings to form a tensor\n",
    "    mean_pooled_embeddings = torch.stack(mean_pooled_embeddings, dim=0)\n",
    "    \n",
    "    ################################################\n",
    "    file_name = f\"emb_{start_idx}_{end_idx}.pickle\"\n",
    "    # Save the mean-pooled embeddings as a pickle object\n",
    "    save_dir = \"18_ViTEmbeddings\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    file_path = os.path.join(save_dir, file_name)\n",
    "    \n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(mean_pooled_embeddings, f)\n",
    "        \n",
    "    ################################################   \n",
    "    file_name = f\"label_{start_idx}_{end_idx}.pickle\"\n",
    "    # Save the mean-pooled embeddings as a pickle object\n",
    "    save_dir = \"18_ViTEmbeddings\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    file_path = os.path.join(save_dir, file_name)\n",
    "    \n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(lbls, f)\n",
    "    #################################################\n",
    "    print(f'Batch_{start_idx}_{end_idx} Completed')\n",
    "\n",
    "    del mean_pooled_embeddings, mean_pooled_embedding, lbls, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ab1e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_0_100 Completed\n",
      "Batch_100_200 Completed\n",
      "Batch_200_300 Completed\n",
      "Batch_300_400 Completed\n",
      "Batch_400_500 Completed\n",
      "Batch_500_600 Completed\n",
      "Batch_600_700 Completed\n",
      "Batch_700_800 Completed\n",
      "Batch_800_900 Completed\n",
      "Batch_900_1000 Completed\n",
      "Batch_1000_1100 Completed\n",
      "Batch_1100_1200 Completed\n",
      "Batch_1200_1300 Completed\n",
      "Batch_1300_1400 Completed\n",
      "Batch_1400_1500 Completed\n",
      "Batch_1500_1600 Completed\n",
      "Batch_1600_1700 Completed\n",
      "Batch_1700_1800 Completed\n",
      "Batch_1800_1900 Completed\n",
      "Batch_1900_2000 Completed\n",
      "Batch_2000_2100 Completed\n",
      "Batch_2100_2200 Completed\n",
      "Batch_2200_2300 Completed\n",
      "Batch_2300_2400 Completed\n",
      "Batch_2400_2500 Completed\n",
      "Batch_2500_2600 Completed\n",
      "Batch_2600_2700 Completed\n",
      "Batch_2700_2800 Completed\n",
      "Batch_2800_2900 Completed\n",
      "Batch_2900_3000 Completed\n",
      "Batch_3000_3100 Completed\n",
      "Batch_3100_3200 Completed\n",
      "Batch_3200_3300 Completed\n",
      "Batch_3300_3400 Completed\n",
      "Batch_3400_3500 Completed\n",
      "Batch_3500_3600 Completed\n",
      "Batch_3600_3700 Completed\n",
      "Batch_3700_3800 Completed\n",
      "Batch_3800_3900 Completed\n",
      "Batch_3900_4000 Completed\n",
      "Batch_4000_4100 Completed\n",
      "Batch_4100_4200 Completed\n",
      "Batch_4200_4300 Completed\n",
      "Batch_4300_4400 Completed\n",
      "Batch_4400_4500 Completed\n",
      "Batch_4500_4600 Completed\n",
      "Batch_4600_4700 Completed\n",
      "Batch_4700_4800 Completed\n",
      "Batch_4800_4900 Completed\n",
      "Batch_4900_5000 Completed\n",
      "Batch_5000_5100 Completed\n",
      "Batch_5100_5200 Completed\n",
      "Batch_5200_5300 Completed\n",
      "Batch_5300_5400 Completed\n",
      "Batch_5400_5500 Completed\n",
      "Batch_5500_5600 Completed\n",
      "Batch_5600_5700 Completed\n",
      "Batch_5700_5800 Completed\n",
      "Batch_5800_5900 Completed\n",
      "Batch_5900_6000 Completed\n",
      "Batch_6000_6100 Completed\n",
      "Batch_6100_6200 Completed\n",
      "Batch_6200_6300 Completed\n",
      "Batch_6300_6400 Completed\n",
      "Batch_6400_6500 Completed\n",
      "Batch_6500_6600 Completed\n",
      "Batch_6600_6700 Completed\n",
      "Batch_6700_6800 Completed\n",
      "Batch_6800_6900 Completed\n",
      "Batch_6900_7000 Completed\n",
      "Batch_7000_7100 Completed\n",
      "Batch_7100_7200 Completed\n",
      "Batch_7200_7300 Completed\n",
      "Batch_7300_7400 Completed\n",
      "Batch_7400_7500 Completed\n",
      "Batch_7500_7600 Completed\n",
      "Batch_7600_7700 Completed\n",
      "Batch_7700_7800 Completed\n",
      "Batch_7800_7900 Completed\n",
      "Batch_7900_8000 Completed\n",
      "Batch_8000_8100 Completed\n",
      "Batch_8100_8200 Completed\n",
      "Batch_8200_8300 Completed\n",
      "Batch_8300_8400 Completed\n",
      "Batch_8400_8500 Completed\n",
      "Batch_8500_8600 Completed\n",
      "Batch_8600_8700 Completed\n",
      "Batch_8700_8800 Completed\n",
      "Batch_8800_8900 Completed\n",
      "Batch_8900_9000 Completed\n",
      "Batch_9000_9100 Completed\n",
      "Batch_9100_9200 Completed\n",
      "Batch_9200_9300 Completed\n",
      "Batch_9300_9400 Completed\n",
      "Batch_9400_9500 Completed\n",
      "Batch_9500_9600 Completed\n",
      "Batch_9600_9700 Completed\n",
      "Batch_9700_9800 Completed\n",
      "Batch_9800_9900 Completed\n",
      "Batch_9900_10000 Completed\n",
      "Batch_10000_10100 Completed\n",
      "Batch_10100_10200 Completed\n",
      "Batch_10200_10300 Completed\n",
      "Batch_10300_10400 Completed\n",
      "Batch_10400_10500 Completed\n",
      "Batch_10500_10600 Completed\n",
      "Batch_10600_10700 Completed\n",
      "Batch_10700_10800 Completed\n",
      "Batch_10800_10900 Completed\n",
      "Batch_10900_11000 Completed\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "total_images = len(total_img_info)\n",
    "for i in range(0, total_images, batch_size):\n",
    "    batch = total_img_info[i : i + batch_size]\n",
    "    process_batch(batch, i, i + batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
