{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db39ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 01:46:46.766783: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import nltk\n",
    "import transformers\n",
    "from nltk.corpus import stopwords\n",
    "sw_nltk = stopwords.words('english')\n",
    "from gensim.models import Word2Vec, word2vec\n",
    "from transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel, AdamW\n",
    "from PIL import Image\n",
    "import natsort\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.collections import PatchCollection\n",
    "import transformers\n",
    "import os\n",
    "import pandas as pd\n",
    "import natsort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import clip\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import os\n",
    "import pandas as pd\n",
    "import natsort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "from keras.losses import mse\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.layers import Dropout\n",
    "TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15da5cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_classification_report(report, mod, exec_time):\n",
    "    lines = report.split('\\n')\n",
    "    data = lines[2:4] + lines[5:9]\n",
    "    data = [line.split() for line in data]\n",
    "    \n",
    "\n",
    "    acc = float(data[2][1])\n",
    "    m_prec = float(data[3][2])\n",
    "    m_recall = float(data[3][3])\n",
    "    m_f1 = float(data[3][4])\n",
    "    w_prec = float(data[4][2])\n",
    "    w_recall = float(data[4][3])\n",
    "    w_f1 = float(data[4][4])\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Model': [mod],\n",
    "        'Accuracy': [acc],\n",
    "        'M-Precision': [m_prec],\n",
    "        'M-Recall': [m_recall],\n",
    "        'M-F1-Score': [m_f1],\n",
    "        'W-Precision': [w_prec],\n",
    "        'W-Recall': [w_recall],\n",
    "        'W-F1-Score': [w_f1],\n",
    "        'Runtime': [exec_time]\n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1157f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of common strings 9986\n",
      "filtered_df shape (9986, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>Milk Milk.zip</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>-What are you doing? -you told me to satanize ...</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>imgflip.com ME 1254 NEW BUGS AFTER CHANGES BUG...</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>Bedroom Kitchen Bathroom Bron memes storage</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.jpg</td>\n",
       "      <td>WAKEUP EARLY FREELANCERS</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>15002.jpg</td>\n",
       "      <td>WAITING FOR THE END OF THE COVID  imgflip.com</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>15003.jpg</td>\n",
       "      <td>SMART WOMEN ARE AROUND  imgflip.com</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>15004.jpg</td>\n",
       "      <td>GOOD GIRLS ARE BEHIND THE CORNER  imgflip.com</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>15005.jpg</td>\n",
       "      <td>COOKING FOR MY WIFE  imgflip.com</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>15006.jpg</td>\n",
       "      <td>LISTEN TOMORROW WILL BE MONDAY imgflip.com FRO...</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9986 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_name                                               text  \\\n",
       "0         1.jpg                                      Milk Milk.zip   \n",
       "1         2.jpg  -What are you doing? -you told me to satanize ...   \n",
       "2         3.jpg  imgflip.com ME 1254 NEW BUGS AFTER CHANGES BUG...   \n",
       "3         4.jpg        Bedroom Kitchen Bathroom Bron memes storage   \n",
       "4         5.jpg                           WAKEUP EARLY FREELANCERS   \n",
       "...         ...                                                ...   \n",
       "9981  15002.jpg      WAITING FOR THE END OF THE COVID  imgflip.com   \n",
       "9982  15003.jpg                SMART WOMEN ARE AROUND  imgflip.com   \n",
       "9983  15004.jpg      GOOD GIRLS ARE BEHIND THE CORNER  imgflip.com   \n",
       "9984  15005.jpg                   COOKING FOR MY WIFE  imgflip.com   \n",
       "9985  15006.jpg  LISTEN TOMORROW WILL BE MONDAY imgflip.com FRO...   \n",
       "\n",
       "            label  \n",
       "0     non_misogyn  \n",
       "1     non_misogyn  \n",
       "2     non_misogyn  \n",
       "3     non_misogyn  \n",
       "4     non_misogyn  \n",
       "...           ...  \n",
       "9981  non_misogyn  \n",
       "9982  non_misogyn  \n",
       "9983  non_misogyn  \n",
       "9984  non_misogyn  \n",
       "9985  non_misogyn  \n",
       "\n",
       "[9986 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>15001.jpg</td>\n",
       "      <td>G HIS. UNDYING FIDELITY Steve is hot and perfe...</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>15002.jpg</td>\n",
       "      <td>How limagined myself as a Teacher...... How I ...</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>15004.jpg</td>\n",
       "      <td>WHERE WILL YOU BE WHEN DIARRHEA STRIKE memecen...</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>15005.jpg</td>\n",
       "      <td>A MAN WITH DREAMS... NEEDS A WOMAN WITH VISION</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>15006.jpg</td>\n",
       "      <td>THIS IS HOW YOUR GIRLFRIEND SEES YOUR FEMALE F...</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>17078.jpg</td>\n",
       "      <td>There are multiple reasons to lower your car T...</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>17079.jpg</td>\n",
       "      <td>MICHELLE OBAMA IS A MAN IGUARANTEE IT makeamem...</td>\n",
       "      <td>misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>17080.jpg</td>\n",
       "      <td>Looks like the airbags deployed 1234498</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>17081.jpg</td>\n",
       "      <td>Half woman half horse</td>\n",
       "      <td>non_misogyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>17082.jpg</td>\n",
       "      <td>We don't mind if a man tries to rape you. G&amp;P ...</td>\n",
       "      <td>misogyn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_name                                               text        label\n",
       "436  15001.jpg  G HIS. UNDYING FIDELITY Steve is hot and perfe...  non_misogyn\n",
       "854  15002.jpg  How limagined myself as a Teacher...... How I ...  non_misogyn\n",
       "743  15004.jpg  WHERE WILL YOU BE WHEN DIARRHEA STRIKE memecen...  non_misogyn\n",
       "44   15005.jpg     A MAN WITH DREAMS... NEEDS A WOMAN WITH VISION  non_misogyn\n",
       "873  15006.jpg  THIS IS HOW YOUR GIRLFRIEND SEES YOUR FEMALE F...  non_misogyn\n",
       "..         ...                                                ...          ...\n",
       "871  17078.jpg  There are multiple reasons to lower your car T...  non_misogyn\n",
       "683  17079.jpg  MICHELLE OBAMA IS A MAN IGUARANTEE IT makeamem...      misogyn\n",
       "406  17080.jpg            Looks like the airbags deployed 1234498  non_misogyn\n",
       "733  17081.jpg                              Half woman half horse  non_misogyn\n",
       "241  17082.jpg  We don't mind if a man tries to rape you. G&P ...      misogyn\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "\n",
    "#  DistilBERT Embeddings (Text Only); (Train and Test Data Embeddings)\n",
    "\n",
    "train2 = pd.read_csv('./training1.csv', delimiter='\\t')\n",
    "\n",
    "IDs = []  #set1\n",
    "images = []\n",
    "directory = 'TRAINING'   # directory where we have images \n",
    "filenames = natsort.natsorted(os.listdir(directory))  \n",
    "\n",
    "# get the ids from the images, where images are having three channels; omit images if channels != 3\n",
    "for i, filename in enumerate(filenames):\n",
    "#     print(i, filename)\n",
    "    if filename.endswith(\".jpg\"):\n",
    "#         ID = int(filename[:-4])\n",
    "        ID = filename\n",
    "        pathname = os.path.join(directory, filename)\n",
    "        im = Image.open(pathname)\n",
    "        im = im.resize((224, 224))  # Resize the image to (224, 224)\n",
    "        imnp = np.array(im)\n",
    "        if len(imnp.shape) != 3:\n",
    "#             print(\"This is 1 channel, so we omit it\", imnp.shape, filename)\n",
    "            continue\n",
    "        IDs.append(ID)\n",
    "        images.append(imnp)\n",
    "\n",
    "def get_common_strings(list1, list2):\n",
    "    return list(set(list1) & set(list2))\n",
    "\n",
    "# Example usage\n",
    "list1 = IDs\n",
    "list2 = list(train2.file_name)  #from the text file where we have text description \n",
    "common_strings = get_common_strings(list1, list2)\n",
    "print('len of common strings', len(common_strings))\n",
    "\n",
    "sorted_ids = natsort.natsorted(common_strings)\n",
    "\n",
    "# print(sorted_ids)\n",
    "\n",
    "# Sort the dataframe with natural ordering of the IDs\n",
    "train2['prefix_file_name'] = train2['file_name'].str.extract('(\\d+)').astype(int)\n",
    "# Assuming 'df' is your DataFrame\n",
    "sorted_train_df = train2.sort_values(by='prefix_file_name', ascending=True)\n",
    "sorted_train_df\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'common_strings' is the list of strings\n",
    "# Get the common string values in the column \n",
    "filtered_df = sorted_train_df[sorted_train_df['file_name'].isin(sorted_ids)].reset_index(drop=True)\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "print('filtered_df shape', filtered_df.shape)\n",
    "\n",
    "train3 = filtered_df.copy()\n",
    "# print(train3.shape)\n",
    "\n",
    "trainx = train3.rename(columns={'Text Transcription': 'text'})\n",
    "# display(trainx)\n",
    "\n",
    "\n",
    "\n",
    "### LOAD TEST DATA\n",
    "\n",
    "test1 = pd.read_csv('Test.csv', delimiter='\\t')\n",
    "test_labels = pd.read_csv('test_labels.txt', \n",
    "                          delimiter='\\t',\n",
    "                         header=None)\n",
    "\n",
    "test_labels.columns = ['file_name', \n",
    "                      \"misogynous\",\n",
    "                       \"shaming\",\n",
    "                       \"stereotype\",\n",
    "                       \"objectification\",\n",
    "                       \"violence\"]\n",
    "\n",
    "merged_test = pd.merge(test1, test_labels, on='file_name', how='inner')\n",
    "\n",
    "\n",
    "# Sort the dataframe with natural ordering of the IDs\n",
    "merged_test['prefix_file_name'] = merged_test['file_name'].str.extract('(\\d+)').astype(int)\n",
    "# Assuming 'df' is your DataFrame\n",
    "merged_test1 = merged_test.sort_values(by='prefix_file_name', ascending=True)\n",
    "merged_test1\n",
    "\n",
    "\n",
    "# # train = train.rename(columns={'Text Transcription': 'text'})\n",
    "test2 = merged_test1.rename(columns={'Text Transcription': 'text'})\n",
    "# test2\n",
    "\n",
    "\n",
    "\n",
    "# ######################################################\n",
    "train = trainx[['file_name', 'text', 'misogynous']]\n",
    "train = train.rename(columns = {'misogynous':'label'})\n",
    "\n",
    "test = test2[['file_name', 'text', 'misogynous']]\n",
    "test = test.rename(columns = {'misogynous':'label'})\n",
    "\n",
    "train['label'] = train['label'].map({0: 'non_misogyn', 1: 'misogyn'})\n",
    "test['label'] = test['label'].map({0: 'non_misogyn', 1: 'misogyn'})\n",
    "display(train)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2666428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeddings shape: (9986, 64)\n",
      "test embeddings shape: (1000, 64)\n",
      "transformer version: 4.15.0\n",
      "cuda\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "len of common strings 9986\n",
      "torch.Size([9986, 512])\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          65664       ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 64)           4160        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 64)           4160        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 64)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, 512)          78528       ['sampling[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 160,768\n",
      "Trainable params: 160,768\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 01:57:10.738992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-05 01:57:10.761028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-05 01:57:10.761158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-05 01:57:10.761518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-05 01:57:10.763072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-05 01:57:10.763191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-05 01:57:10.763272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-05 01:57:10.763487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-05 01:57:10.763577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-05 01:57:10.763659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-05 01:57:10.763739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4604 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "107/313 [=========>....................] - ETA: 0s - loss: 95.0048 - kl_loss: 1.0745 - mse_loss: 93.9303  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 01:57:11.517498: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 83.0524 - kl_loss: 0.6605 - mse_loss: 82.3920\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 74.5083 - kl_loss: 0.5428 - mse_loss: 73.9654\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 73.1438 - kl_loss: 0.6439 - mse_loss: 72.5000\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 72.2523 - kl_loss: 0.7296 - mse_loss: 71.5227\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 71.6197 - kl_loss: 0.8034 - mse_loss: 70.8164\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 71.2125 - kl_loss: 0.8381 - mse_loss: 70.3744\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 70.6954 - kl_loss: 0.8889 - mse_loss: 69.8065\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 70.3144 - kl_loss: 0.9177 - mse_loss: 69.3967\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 70.0202 - kl_loss: 0.9453 - mse_loss: 69.0748\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 69.7477 - kl_loss: 0.9570 - mse_loss: 68.7907\n",
      "313/313 [==============================] - 0s 525us/step\n",
      "(9986, 64)\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.3 GB\n",
      "Cached:    0.4 GB\n",
      "len of common strings 1000\n",
      "32/32 [==============================] - 0s 639us/step\n",
      "Image Test Embeddings Dimension (1000, 64)\n",
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "transformer version: 4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion Embeddings Train (9986, 192)\n",
      "Fusion Embeddings Test (1000, 192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nitesh/env/dev38/python38/lib/python3.8/site-packages/xgboost/compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     misogyn       0.64      0.91      0.75       500\n",
      " non_misogyn       0.85      0.49      0.62       500\n",
      "\n",
      "    accuracy                           0.70      1000\n",
      "   macro avg       0.74      0.70      0.69      1000\n",
      "weighted avg       0.74      0.70      0.69      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     misogyn       0.64      0.91      0.75       500\n",
      " non_misogyn       0.85      0.49      0.62       500\n",
      "\n",
      "    accuracy                           0.70      1000\n",
      "   macro avg       0.74      0.70      0.69      1000\n",
      "weighted avg       0.74      0.70      0.69      1000\n",
      "\n",
      "Execution time: 254.94431853294373 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeddings shape: (9986, 64)\n",
      "test embeddings shape: (1000, 64)\n",
      "transformer version: 4.15.0\n",
      "cuda\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.4 GB\n",
      "len of common strings 9986\n",
      "torch.Size([9986, 512])\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          65664       ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128)          0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64)           8256        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 64)           4160        ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 64)           4160        ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 64)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, 512)          78528       ['sampling_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 160,768\n",
      "Trainable params: 160,768\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 82.7056 - kl_loss: 0.7225 - mse_loss: 81.9830\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 74.3188 - kl_loss: 0.5710 - mse_loss: 73.7478\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 72.9451 - kl_loss: 0.6889 - mse_loss: 72.2561\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 72.0767 - kl_loss: 0.7841 - mse_loss: 71.2926\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 71.4407 - kl_loss: 0.8601 - mse_loss: 70.5806\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 70.9127 - kl_loss: 0.9048 - mse_loss: 70.0078\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 70.5460 - kl_loss: 0.9509 - mse_loss: 69.5951\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 70.2003 - kl_loss: 0.9747 - mse_loss: 69.2257\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 69.9391 - kl_loss: 0.9856 - mse_loss: 68.9534\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 69.7194 - kl_loss: 0.9950 - mse_loss: 68.7243\n",
      "313/313 [==============================] - 0s 532us/step\n",
      "(9986, 64)\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.3 GB\n",
      "Cached:    0.4 GB\n",
      "len of common strings 1000\n",
      "32/32 [==============================] - 0s 692us/step\n",
      "Image Test Embeddings Dimension (1000, 64)\n",
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "transformer version: 4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion Embeddings Train (9986, 192)\n",
      "Fusion Embeddings Test (1000, 192)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     misogyn       0.63      0.93      0.76       500\n",
      " non_misogyn       0.87      0.46      0.60       500\n",
      "\n",
      "    accuracy                           0.70      1000\n",
      "   macro avg       0.75      0.70      0.68      1000\n",
      "weighted avg       0.75      0.70      0.68      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     misogyn       0.63      0.93      0.76       500\n",
      " non_misogyn       0.87      0.46      0.60       500\n",
      "\n",
      "    accuracy                           0.70      1000\n",
      "   macro avg       0.75      0.70      0.68      1000\n",
      "weighted avg       0.75      0.70      0.68      1000\n",
      "\n",
      "Execution time: 267.3105022907257 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeddings shape: (9986, 64)\n",
      "test embeddings shape: (1000, 64)\n",
      "transformer version: 4.15.0\n",
      "cuda\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.4 GB\n",
      "len of common strings 9986\n",
      "torch.Size([9986, 512])\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 128)          65664       ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 128)          0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64)           8256        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 64)           4160        ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 64)           4160        ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " sampling_2 (Sampling)          (None, 64)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, 512)          78528       ['sampling_2[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 160,768\n",
      "Trainable params: 160,768\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 82.5115 - kl_loss: 0.6916 - mse_loss: 81.8199\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 74.1843 - kl_loss: 0.5858 - mse_loss: 73.5985\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 72.7699 - kl_loss: 0.7127 - mse_loss: 72.0572\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 71.9541 - kl_loss: 0.7899 - mse_loss: 71.1641\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 71.3581 - kl_loss: 0.8503 - mse_loss: 70.5078\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 70.9909 - kl_loss: 0.8879 - mse_loss: 70.1029\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 70.6256 - kl_loss: 0.9163 - mse_loss: 69.7094\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 70.3110 - kl_loss: 0.9443 - mse_loss: 69.3668\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 70.0777 - kl_loss: 0.9662 - mse_loss: 69.1115\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 69.8077 - kl_loss: 0.9841 - mse_loss: 68.8236\n",
      "313/313 [==============================] - 0s 484us/step\n",
      "(9986, 64)\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.3 GB\n",
      "Cached:    0.4 GB\n",
      "len of common strings 1000\n",
      "32/32 [==============================] - 0s 703us/step\n",
      "Image Test Embeddings Dimension (1000, 64)\n",
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "transformer version: 4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion Embeddings Train (9986, 192)\n",
      "Fusion Embeddings Test (1000, 192)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     misogyn       0.64      0.93      0.76       500\n",
      " non_misogyn       0.88      0.49      0.63       500\n",
      "\n",
      "    accuracy                           0.71      1000\n",
      "   macro avg       0.76      0.71      0.69      1000\n",
      "weighted avg       0.76      0.71      0.69      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     misogyn       0.64      0.93      0.76       500\n",
      " non_misogyn       0.88      0.49      0.63       500\n",
      "\n",
      "    accuracy                           0.71      1000\n",
      "   macro avg       0.76      0.71      0.69      1000\n",
      "weighted avg       0.76      0.71      0.69      1000\n",
      "\n",
      "Execution time: 272.4992747306824 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeddings shape: (9986, 64)\n",
      "test embeddings shape: (1000, 64)\n",
      "transformer version: 4.15.0\n",
      "cuda\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.4 GB\n",
      "len of common strings 9986\n",
      "torch.Size([9986, 512])\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 128)          65664       ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 128)          0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 64)           8256        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 64)           4160        ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 64)           4160        ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " sampling_3 (Sampling)          (None, 64)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, 512)          78528       ['sampling_3[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 160,768\n",
      "Trainable params: 160,768\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 82.4728 - kl_loss: 0.6808 - mse_loss: 81.7919\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 74.3673 - kl_loss: 0.5493 - mse_loss: 73.8180\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 72.9951 - kl_loss: 0.6694 - mse_loss: 72.3258\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 72.1691 - kl_loss: 0.7612 - mse_loss: 71.4079\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 71.5377 - kl_loss: 0.8271 - mse_loss: 70.7106\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 71.1053 - kl_loss: 0.8756 - mse_loss: 70.2297\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 70.6922 - kl_loss: 0.9060 - mse_loss: 69.7861\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 70.3230 - kl_loss: 0.9318 - mse_loss: 69.3913\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 70.1281 - kl_loss: 0.9447 - mse_loss: 69.1834\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 69.8735 - kl_loss: 0.9582 - mse_loss: 68.9153\n",
      "313/313 [==============================] - 0s 508us/step\n",
      "(9986, 64)\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.3 GB\n",
      "Cached:    0.4 GB\n",
      "len of common strings 1000\n",
      "32/32 [==============================] - 0s 565us/step\n",
      "Image Test Embeddings Dimension (1000, 64)\n",
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "transformer version: 4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion Embeddings Train (9986, 192)\n",
      "Fusion Embeddings Test (1000, 192)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     misogyn       0.64      0.93      0.76       500\n",
      " non_misogyn       0.87      0.49      0.63       500\n",
      "\n",
      "    accuracy                           0.71      1000\n",
      "   macro avg       0.76      0.71      0.69      1000\n",
      "weighted avg       0.76      0.71      0.69      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     misogyn       0.64      0.93      0.76       500\n",
      " non_misogyn       0.87      0.49      0.63       500\n",
      "\n",
      "    accuracy                           0.71      1000\n",
      "   macro avg       0.76      0.71      0.69      1000\n",
      "weighted avg       0.76      0.71      0.69      1000\n",
      "\n",
      "Execution time: 257.5518820285797 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeddings shape: (9986, 64)\n",
      "test embeddings shape: (1000, 64)\n",
      "transformer version: 4.15.0\n",
      "cuda\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.4 GB\n",
      "len of common strings 9986\n",
      "torch.Size([9986, 512])\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 128)          65664       ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 64)           8256        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 64)           4160        ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 64)           4160        ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " sampling_4 (Sampling)          (None, 64)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, 512)          78528       ['sampling_4[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 160,768\n",
      "Trainable params: 160,768\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 82.2581 - kl_loss: 0.7853 - mse_loss: 81.4729\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 74.2081 - kl_loss: 0.6023 - mse_loss: 73.6058\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 72.8181 - kl_loss: 0.7248 - mse_loss: 72.0933\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 71.9783 - kl_loss: 0.8025 - mse_loss: 71.1759\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 71.3775 - kl_loss: 0.8674 - mse_loss: 70.5101\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 70.9393 - kl_loss: 0.9134 - mse_loss: 70.0259\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 70.5526 - kl_loss: 0.9503 - mse_loss: 69.6023\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 70.2421 - kl_loss: 0.9753 - mse_loss: 69.2668\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 69.9297 - kl_loss: 0.9908 - mse_loss: 68.9388\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 69.6526 - kl_loss: 1.0069 - mse_loss: 68.6456\n",
      "313/313 [==============================] - 0s 561us/step\n",
      "(9986, 64)\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.3 GB\n",
      "Cached:    0.4 GB\n",
      "len of common strings 1000\n",
      "32/32 [==============================] - 0s 650us/step\n",
      "Image Test Embeddings Dimension (1000, 64)\n",
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "transformer version: 4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion Embeddings Train (9986, 192)\n",
      "Fusion Embeddings Test (1000, 192)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     misogyn       0.64      0.91      0.75       500\n",
      " non_misogyn       0.84      0.48      0.61       500\n",
      "\n",
      "    accuracy                           0.69      1000\n",
      "   macro avg       0.74      0.69      0.68      1000\n",
      "weighted avg       0.74      0.69      0.68      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     misogyn       0.64      0.91      0.75       500\n",
      " non_misogyn       0.84      0.48      0.61       500\n",
      "\n",
      "    accuracy                           0.69      1000\n",
      "   macro avg       0.74      0.69      0.68      1000\n",
      "weighted avg       0.74      0.69      0.68      1000\n",
      "\n",
      "Execution time: 265.1270577907562 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeddings shape: (9986, 64)\n",
      "test embeddings shape: (1000, 64)\n",
      "transformer version: 4.15.0\n",
      "cuda\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.4 GB\n",
      "len of common strings 9986\n",
      "torch.Size([9986, 512])\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 128)          65664       ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 128)          0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 64)           8256        ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 64)           4160        ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 64)           4160        ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " sampling_5 (Sampling)          (None, 64)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, 512)          78528       ['sampling_5[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 160,768\n",
      "Trainable params: 160,768\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 83.2214 - kl_loss: 0.7450 - mse_loss: 82.4764\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 74.4219 - kl_loss: 0.5648 - mse_loss: 73.8571\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 73.0871 - kl_loss: 0.6662 - mse_loss: 72.4209\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 72.1185 - kl_loss: 0.7596 - mse_loss: 71.3589\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 71.4775 - kl_loss: 0.8280 - mse_loss: 70.6496\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 70.9849 - kl_loss: 0.8808 - mse_loss: 70.1041\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 70.5615 - kl_loss: 0.9215 - mse_loss: 69.6400\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 70.2434 - kl_loss: 0.9453 - mse_loss: 69.2981\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 69.9400 - kl_loss: 0.9591 - mse_loss: 68.9810\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 69.7273 - kl_loss: 0.9704 - mse_loss: 68.7569\n",
      "313/313 [==============================] - 0s 522us/step\n",
      "(9986, 64)\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.3 GB\n",
      "Cached:    0.4 GB\n",
      "len of common strings 1000\n",
      "32/32 [==============================] - 0s 638us/step\n",
      "Image Test Embeddings Dimension (1000, 64)\n",
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "transformer version: 4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion Embeddings Train (9986, 192)\n",
      "Fusion Embeddings Test (1000, 192)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     misogyn       0.65      0.93      0.77       500\n",
      " non_misogyn       0.88      0.50      0.63       500\n",
      "\n",
      "    accuracy                           0.71      1000\n",
      "   macro avg       0.76      0.71      0.70      1000\n",
      "weighted avg       0.76      0.71      0.70      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     misogyn       0.65      0.93      0.77       500\n",
      " non_misogyn       0.88      0.50      0.63       500\n",
      "\n",
      "    accuracy                           0.71      1000\n",
      "   macro avg       0.76      0.71      0.70      1000\n",
      "weighted avg       0.76      0.71      0.70      1000\n",
      "\n",
      "Execution time: 261.7795150279999 seconds\n"
     ]
    }
   ],
   "source": [
    "### TRAINING\n",
    "\n",
    "df_BLPCLPXGB = pd.DataFrame(columns=['Model', 'Accuracy', \n",
    "                       'M-Precision', 'M-Recall', \n",
    "                       'M-F1-Score', 'W-Precision',\n",
    "                       'W-Recall', 'W-F1-Score', 'Runtime'])\n",
    "\n",
    "\n",
    "for KK in list([7,10,18,22,55,77]):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    random.seed(KK)\n",
    "    np.random.seed(KK)\n",
    "    torch.manual_seed(KK)\n",
    "    tf.random.set_seed(KK)\n",
    "\n",
    "\n",
    "    device = \"cpu\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model.to(device)\n",
    "    projection_layer = torch.nn.Linear(768, 64)  # Add a linear layer to project embeddings to size 64\n",
    "    projection_layer.to(device)\n",
    "\n",
    "    train_encodings = tokenizer(\n",
    "        train[\"text\"].tolist(),\n",
    "        padding=True,\n",
    "        max_length=20,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    test_encodings = tokenizer(\n",
    "        test[\"text\"].tolist(),\n",
    "        padding=True,\n",
    "        max_length=20,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(**train_encodings.to(device))\n",
    "        train_embeddings = train_outputs.last_hidden_state[:, 0, :]  # Extract embeddings for the [CLS] token\n",
    "        train_embeddings = projection_layer(train_embeddings)  # Project embeddings to size 65\n",
    "\n",
    "        test_outputs = model(**test_encodings.to(device))\n",
    "        test_embeddings = test_outputs.last_hidden_state[:, 0, :]  # Extract embeddings for the [CLS] token\n",
    "        test_embeddings = projection_layer(test_embeddings)  # Project embeddings to size 65\n",
    "\n",
    "    train_embeddings = train_embeddings.cpu().numpy()\n",
    "    test_embeddings = test_embeddings.cpu().numpy()\n",
    "\n",
    "    print(\"train embeddings shape:\", train_embeddings.shape)\n",
    "    print(\"test embeddings shape:\", test_embeddings.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # CLIP Image Embeddings (Training Image Embeddings)\n",
    "\n",
    "    print('transformer version:', transformers.__version__)\n",
    "    # !git clone https://github.com/salesforce/BLIP\n",
    "    # Ref: https://github.com/salesforce/BLIP\n",
    "\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    #Additional Info when using cuda\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "    #  watch -n 2 nvidia-smi\n",
    "\n",
    "\n",
    "    # model_16\n",
    "\n",
    "    # Define the directory containing the images\n",
    "    IDs = []  #set1\n",
    "    images = []\n",
    "    image_dir = \"./TRAINING\"\n",
    "    # # Load the data\n",
    "    train2 = pd.read_csv('./training1.csv', delimiter='\\t')\n",
    "\n",
    "    # Get a list of image filenames in the directory\n",
    "    filenames = natsort.natsorted(os.listdir(image_dir))\n",
    "    # len(image_filenames)\n",
    "\n",
    "    # Correct the images as some images are not having 3 channels.Omit such images.\n",
    "    # get the ids from the images, where images are having three channels; omit images if channels != 3\n",
    "    for i, filename in enumerate(filenames):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "    #         ID = int(filename[:-4])\n",
    "            ID = filename\n",
    "            pathname = os.path.join(image_dir, filename)\n",
    "            im = Image.open(pathname)\n",
    "    #         print(im.size)\n",
    "            im = im.resize((224, 224))  # Resize the image to (224, 224)\n",
    "            imnp = np.array(im)\n",
    "            if len(imnp.shape) != 3:\n",
    "    #             print(\"This is 1 channel, so we omit it\", imnp.shape, filename)\n",
    "                continue\n",
    "            IDs.append(ID)\n",
    "            images.append(imnp)\n",
    "\n",
    "\n",
    "    # Align the image and textual data and extract where we have both available \n",
    "    def get_common_strings(list1, list2):\n",
    "        return list(set(list1) & set(list2))\n",
    "\n",
    "    # Example usage\n",
    "    list1 = IDs\n",
    "    list2 = list(train2.file_name)  #from the text file where we have text description \n",
    "    common_strings = get_common_strings(list1, list2)\n",
    "    print('len of common strings', len(common_strings))\n",
    "\n",
    "    image_filenames = natsort.natsorted(common_strings)\n",
    "    # display(image_filenames)\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    train_labelset = train2[['file_name', 'misogynous']]\n",
    "    IDs_df = pd.DataFrame({'file_name': image_filenames})\n",
    "    joined_df = IDs_df.merge(train_labelset, on='file_name', how='left')\n",
    "    misogynous_labels = joined_df['misogynous'].tolist()\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "\n",
    "\n",
    "    # Create an empty list to store the image features\n",
    "    all_image_features_16 = []\n",
    "\n",
    "\n",
    "    model_16, preprocess_16 = clip.load('ViT-B/16', device=device)\n",
    "\n",
    "    # Process each image and extract features\n",
    "    for filename in image_filenames:\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "\n",
    "        # Preprocess the image\n",
    "        image_16 = preprocess_16(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "        # Encode the image features\n",
    "        with torch.no_grad():\n",
    "            image_features_16 = model_16.encode_image(image_16)\n",
    "\n",
    "        # Append the image features to the list\n",
    "        all_image_features_16.append(image_features_16)\n",
    "\n",
    "    # Concatenate the image features into a single tensor\n",
    "    all_image_features_16 = torch.cat(all_image_features_16, dim=0)\n",
    "    print(all_image_features_16.shape)\n",
    "    # torch.Size([9986, 512])\n",
    "\n",
    "\n",
    "\n",
    "    ## REDUCE 512 EMBEDDING SIZE TO 64 EMBEDDING SIZE #####\n",
    "    #######################################################\n",
    "\n",
    "    random.seed(KK)\n",
    "    np.random.seed(KK)\n",
    "    torch.manual_seed(KK)\n",
    "    tf.random.set_seed(KK)\n",
    "\n",
    "    embedding_size = 64\n",
    "    all_image_features_16_cpu = all_image_features_16.cpu()\n",
    "    all_image_features_16_numpy = all_image_features_16_cpu.numpy()\n",
    "\n",
    "\n",
    "\n",
    "    ## using the below Sampling class from WK08 lab tutorials. \n",
    "    class Sampling(layers.Layer):\n",
    "\n",
    "\n",
    "        \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\n",
    "            Sampling from the distribution of z_mu and z_var helps in \n",
    "            generating new point in latent space which we can pass to the decoder network \n",
    "            to reconstruct the data samples similar to original input\n",
    "        \"\"\"\n",
    "        def call(self, inputs):\n",
    "\n",
    "            #the encoder network produces a mean and a variance for each dimension of the latent space.\n",
    "            z_mean, z_log_var = inputs\n",
    "\n",
    "            # epsilon is not a weight (ie is not learned) and is not calculated based on \n",
    "            # the output of the previous layer.\n",
    "            # epsilon is just a constant Tensor (a new one, each time we call this layer).\n",
    "\n",
    "\n",
    "            epsilon = K.random_normal(shape=tf.shape(z_mean)) # N(0, 1)\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon # N(mu, sigma)\n",
    "\n",
    "\n",
    "    # for the encoder part, now the original dimension would be the pretrained model output flattened features\n",
    "    # dimension\n",
    "    original_dim = all_image_features_16_numpy.shape[1] \n",
    "    intermediate_dim = 128  \n",
    "    intermediate_dim_1 = 64\n",
    "\n",
    "    latent_dim = embedding_size  # i.e. 18, we set latent dimension to be of embedding size i.e. 18.\n",
    "\n",
    "    # Define Input to the encoder \n",
    "    #Encoder Model Structure\n",
    "    original_inputs = layers.Input(shape=(original_dim,), name=\"encoder_input\")\n",
    "    x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
    "    x = Dropout(0.0001)(x) # add dropout with  tiny probability because data is small\n",
    "    x = layers.Dense(intermediate_dim_1, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()((z_mean, z_log_var))   #updated\n",
    "    encoder = models.Model(inputs=original_inputs, outputs=z, name=\"encoder\") \n",
    "    # encoder = models.Model(inputs=original_inputs, outputs=[z_mean, z_log_var], name=\"encoder\")\n",
    "\n",
    "\n",
    "    #decoder model\n",
    "    latent_inputs = layers.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
    "    x = layers.Dense(intermediate_dim_1, activation=\"relu\")(latent_inputs)\n",
    "    x = Dropout(0.0001)(x) # add dropout with  tiny probability because data is small\n",
    "    x = layers.Dense(intermediate_dim, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
    "    decoder = models.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
    "\n",
    "    # z = Sampling()((z_mean, z_log_var))\n",
    "    outputs = decoder(z)\n",
    "    vae = models.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
    "    vae.summary()\n",
    "\n",
    "    #get the reconstruction loss\n",
    "    reconstruction_loss = mse(original_inputs, outputs) # xhat should match x\n",
    "    reconstruction_loss = original_dim * K.mean(reconstruction_loss)\n",
    "\n",
    "    # regularization KL divergence term encourages the learned latent space to be a smooth manifold.\n",
    "    kl_loss = -0.5 * tf.reduce_mean(\n",
    "        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "    vae.add_loss(kl_loss)\n",
    "    vae.add_metric(kl_loss, name='kl_loss', aggregation='mean')\n",
    "    vae.add_loss(reconstruction_loss)\n",
    "    vae.add_metric(reconstruction_loss, name='mse_loss', aggregation='mean')\n",
    "\n",
    "    # set Adam optimizer with learning ate 0.001\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    #Compile the model \n",
    "    vae.compile(optimizer)\n",
    "\n",
    "    # Train. x_train is both the \"x\" and the \"y\"\n",
    "    # vae.fit(flat_features, flat_features, epochs=10)\n",
    "\n",
    "    random.seed(KK)\n",
    "    np.random.seed(KK)\n",
    "    torch.manual_seed(KK)\n",
    "    tf.random.set_seed(KK)\n",
    "\n",
    "    vae.fit(all_image_features_16_numpy, all_image_features_16_numpy, epochs=10)\n",
    "    # Increasing the epochs performing badly when we run k-neareast neighbour to achieve the \n",
    "    # nearest posters. \n",
    "\n",
    "    #In this code, we first need to build the vae model before using encoder.predict(). \n",
    "    #This is because the encoder model is a part of the vae model and needs to be built first.\n",
    "\n",
    "\n",
    "\n",
    "    latent_emb = encoder.predict(all_image_features_16_numpy)  \n",
    "    # z_mean, z_log_var = encoder.predict(all_image_features_16_numpy)\n",
    "    # latent_emb = Sampling()((z_mean, z_log_var))\n",
    "    print(latent_emb.shape) \n",
    "\n",
    "    #Additional Info when using cuda\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # CLIP Image Embeddings (Test Image Embeddings)\n",
    "\n",
    "    # TEST DATA IMAGES LOADING\n",
    "\n",
    "\n",
    "    TEST_IDs = []  #set1\n",
    "    TEST_images = []\n",
    "    TEST_directory = './TEST'   # directory where we have images \n",
    "    test_filenames = natsort.natsorted(os.listdir(TEST_directory))  \n",
    "\n",
    "    # Correct the images as some images are not having 3 channels.Omit such images.\n",
    "    # get the ids from the images, where images are having three channels; omit images if channels != 3\n",
    "    for i, filename in enumerate(test_filenames):\n",
    "    #     print(i, filename)\n",
    "        if filename.endswith(\".jpg\"):\n",
    "    #         ID = int(filename[:-4])\n",
    "            TEST_ID = filename\n",
    "            pathname = os.path.join(TEST_directory, filename)\n",
    "            im = Image.open(pathname)\n",
    "            im = im.resize((224, 224))  # Resize the image to (224, 224)\n",
    "            TEST_imnp = np.array(im)\n",
    "            if len(TEST_imnp.shape) != 3:\n",
    "    #             print(\"This is 1 channel, so we omit it\", TEST_imnp.shape, filename)\n",
    "                continue\n",
    "            TEST_IDs.append(TEST_ID)\n",
    "            TEST_images.append(TEST_imnp)\n",
    "\n",
    "\n",
    "    # Align the image and textual data and extract where we have both available \n",
    "    def get_common_strings(list1, list2):\n",
    "        return list(set(list1) & set(list2))\n",
    "\n",
    "    # Example usage\n",
    "    TEST_list1 = TEST_IDs\n",
    "    TEST_list2 = list(test2.file_name)  #from the text file where we have text description \n",
    "    TEST_common_strings = get_common_strings(TEST_list1, TEST_list2)\n",
    "    print('len of common strings', len(TEST_common_strings))\n",
    "\n",
    "    TEST_sorted_ids = natsort.natsorted(TEST_common_strings)\n",
    "    # len(TEST_sorted_ids)\n",
    "\n",
    "\n",
    "    TEST_IDs1 = []\n",
    "    TEST_images1 = [] \n",
    "    # Correct the images as some images are not having 3 channels.Omit such images.\n",
    "    # get the ids from the images, where images are having three channels; omit images if channels != 3\n",
    "    for i, filename in enumerate(TEST_sorted_ids):\n",
    "    #     print(i, filename)\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            TEST_ID_0 = filename\n",
    "            pathname = os.path.join(TEST_directory, filename)\n",
    "            im = Image.open(pathname)\n",
    "            im = im.resize((224, 224))  # Resize the image to (224, 224)\n",
    "            TEST_1_imnp = np.array(im)\n",
    "            if len(TEST_1_imnp.shape) != 3:\n",
    "    #             print(\"This is 1 channel, so we omit it\", TEST_1_imnp.shape, filename)\n",
    "                continue\n",
    "    #         print('Filename', TEST_ID_0)\n",
    "            TEST_IDs1.append(TEST_ID_0)\n",
    "            TEST_images1.append(TEST_1_imnp)\n",
    "\n",
    "\n",
    "    len(TEST_images1), len(TEST_IDs1)\n",
    "\n",
    "\n",
    "    #### TEST EMBEDDINGS #####\n",
    "    clip_test_embeddings = []\n",
    "\n",
    "    for image in TEST_images1:\n",
    "        # Preprocess the image\n",
    "        image_tensor = preprocess_16(Image.fromarray(image)).unsqueeze(0).to(device)\n",
    "\n",
    "        # Encode the image features\n",
    "        with torch.no_grad():\n",
    "            image_features = model_16.encode_image(image_tensor)\n",
    "\n",
    "        # Append the image features to the list\n",
    "        clip_test_embeddings.append(image_features)\n",
    "\n",
    "    clip_test_embeddings = torch.cat(clip_test_embeddings, dim=0)\n",
    "\n",
    "    # Move the test embeddings tensor to the CPU\n",
    "    clip_test_embeddings = clip_test_embeddings.cpu()\n",
    "\n",
    "    # Convert the test embeddings tensor to a NumPy array\n",
    "    clip_test_embeddings_np = clip_test_embeddings.numpy()\n",
    "\n",
    "    # Use the existing encoder to reduce the dimensions of the test embeddings\n",
    "    reduced_test_embeddings  = encoder.predict(clip_test_embeddings_np)\n",
    "    # z_mean_clipmod, z_log_var_clipmod = encoder.predict(clip_test_embeddings_np)\n",
    "    # reduced_test_embeddings = Sampling()((z_mean_clipmod, z_log_var_clipmod))\n",
    "\n",
    "    print('Image Test Embeddings Dimension', reduced_test_embeddings.shape)\n",
    "\n",
    "\n",
    "\n",
    "    if os.getcwd() != \"/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\":\n",
    "        %cd BLIP\n",
    "    print(os.getcwd())\n",
    "\n",
    "    # Load the training captions data and test captions data files. \n",
    "    train_capn = pd.read_csv('train_BLIP_captions.csv')\n",
    "\n",
    "    test_capn = pd.read_csv('test_BLIP_captions.csv')\n",
    "\n",
    "\n",
    "\n",
    "    random.seed(KK)\n",
    "    np.random.seed(KK)\n",
    "    torch.manual_seed(KK)\n",
    "    tf.random.set_seed(KK)\n",
    "\n",
    "    print('transformer version:', transformers.__version__)\n",
    "    # !git clone https://github.com/salesforce/BLIP\n",
    "    # Ref: https://github.com/salesforce/BLIP\n",
    "\n",
    "    from transformers import DistilBertTokenizer, DistilBertModel\n",
    "    tokenizer1 = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "    device = \"cpu\"\n",
    "    model1.to(device)\n",
    "    projection_layer1 = torch.nn.Linear(768, 64)  # Add a linear layer to project embeddings to size 64\n",
    "    projection_layer1.to(device)\n",
    "\n",
    "    train_encodings1 = tokenizer1(\n",
    "        train_capn[\"text\"].tolist(),\n",
    "        padding=True,\n",
    "        max_length=20,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    test_encodings1 = tokenizer1(\n",
    "        test_capn[\"text\"].tolist(),\n",
    "        padding=True,\n",
    "        max_length=20,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_outputs1 = model1(**train_encodings1.to(device))\n",
    "        train_embeddings1 = train_outputs1.last_hidden_state[:, 0, :]  # Extract embeddings for the [CLS] token\n",
    "        train_embeddings1 = projection_layer1(train_embeddings1)  # Project embeddings to size 65\n",
    "\n",
    "        test_outputs1 = model1(**test_encodings1.to(device))\n",
    "        test_embeddings1 = test_outputs1.last_hidden_state[:, 0, :]  # Extract embeddings for the [CLS] token\n",
    "        test_embeddings1 = projection_layer1(test_embeddings1)  # Project embeddings to size 65\n",
    "\n",
    "\n",
    "    # Concatenate the text and image embeddings\n",
    "    fusion_embeddings_train = np.concatenate((train_embeddings, latent_emb, train_embeddings1), axis=1)\n",
    "    print('Fusion Embeddings Train', fusion_embeddings_train.shape)\n",
    "\n",
    "\n",
    "    # Concatenate the text and image embeddings\n",
    "    fusion_embeddings_test = np.concatenate((test_embeddings, reduced_test_embeddings, test_embeddings1), axis=1)\n",
    "    print('Fusion Embeddings Test', fusion_embeddings_test.shape)\n",
    "\n",
    "\n",
    "    import pickle\n",
    "    # Save the embeddings as a pickle object\n",
    "    pickle_path_train = f\"/home/nitesh/Documents/MY_THESIS/MAMI/Triple_Fusion_Embeddings/fusion_embeddings_train_{KK}.pkl\"\n",
    "    with open(pickle_path_train, 'wb') as f:\n",
    "        pickle.dump(fusion_embeddings_train, f)\n",
    "\n",
    "    # Save the embeddings as a pickle object\n",
    "    pickle_path_test = f\"/home/nitesh/Documents/MY_THESIS/MAMI/Triple_Fusion_Embeddings/fusion_embeddings_test_{KK}.pkl\"\n",
    "    with open(pickle_path_test, 'wb') as f:\n",
    "        pickle.dump(fusion_embeddings_test, f)\n",
    "\n",
    "\n",
    "\n",
    "    # TRAIN XGBOOST AND CREATE CLASSIFICATION REPORT\n",
    "    import xgboost as xgb\n",
    "    # assert train_embeddings.shape[0] == latent_emb.shape[0]\n",
    "\n",
    "\n",
    "    # Initialize the XGBoost Classifier\n",
    "    xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "    train_lbls = train.label\n",
    "    # Train the classifier\n",
    "    xgb_classifier.fit(fusion_embeddings_train, train_lbls)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    from sklearn.metrics import classification_report \n",
    "\n",
    "    y_pred = xgb_classifier.predict(fusion_embeddings_test)\n",
    "    # test_lbls = test.label\n",
    "    print(classification_report(test[\"label\"].tolist(), y_pred))\n",
    "\n",
    "\n",
    "    report_clip_blip_xgboost = classification_report(test[\"label\"].tolist(), y_pred, digits=3)\n",
    "    print(classification_report(test[\"label\"].tolist(), y_pred))\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time1 = end_time - start_time\n",
    "\n",
    "    print(f\"Execution time: {execution_time1} seconds\")\n",
    "\n",
    "    modname = 'DistilBERT_CLIP_BLIP_XGBOOST'\n",
    "    df1 = export_classification_report(report_clip_blip_xgboost, modname, execution_time1)\n",
    "    df_BLPCLPXGB = pd.concat([df_BLPCLPXGB, df1])\n",
    "\n",
    "\n",
    "\n",
    "    os.chdir(\"/home/nitesh/Documents/MY_THESIS/MAMI\")\n",
    "\n",
    "    del start_time, tokenizer, model, projection_layer\n",
    "    del train_encodings, test_encodings, train_outputs, test_outputs\n",
    "    del train_embeddings, test_embeddings\n",
    "    del IDs, images, image_dir, train2, filenames, common_strings, image_filenames\n",
    "    del train_labelset, IDs_df, joined_df, misogynous_labels\n",
    "    del all_image_features_16, model_16, preprocess_16\n",
    "    del all_image_features_16_cpu, all_image_features_16_numpy\n",
    "    del Sampling, original_dim, intermediate_dim, intermediate_dim_1, latent_dim\n",
    "    del original_inputs, x, z_mean, z_log_var, z, encoder\n",
    "    del latent_inputs, outputs, decoder, vae, optimizer, reconstruction_loss, kl_loss\n",
    "    del latent_emb, TEST_IDs, TEST_images, TEST_directory, test_filenames, TEST_common_strings\n",
    "    del TEST_sorted_ids, TEST_IDs1, TEST_images1, clip_test_embeddings, clip_test_embeddings_np\n",
    "    del reduced_test_embeddings, train_capn, test_capn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b980b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BLPCLPXGB.to_excel('classification_report_DistilBERT_ViT_VAE_BLIP_XGBOOST.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e518b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
