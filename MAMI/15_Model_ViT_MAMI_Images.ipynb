{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19419b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 14:42:46.199724: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch \n",
    "from transformers import ViTImageProcessor\n",
    "from PIL import Image\n",
    "from transformers import ViTForImageClassification\n",
    "from torchvision.transforms import functional as F\n",
    "import natsort\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53ea09ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30007/1435002706.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_txt_file['prefix_file_name'] = sub_txt_file['file_name'].str.extract('(\\d+)').astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of common strings 9986\n",
      "9986 10000 9986\n",
      "len of common strings 1000\n",
      "1000 1000\n",
      "total image info created\n",
      "Results saved as pickle object.\n",
      "total image info created\n",
      "Results saved as pickle object.\n",
      "total image info created\n",
      "Results saved as pickle object.\n"
     ]
    }
   ],
   "source": [
    "PERFORMANCE_RESULTS = pd.DataFrame(columns=['Model', 'EvalSet', 'Accuracy', \n",
    "                       'M-Precision', 'M-Recall', \n",
    "                       'M-F1-Score', 'W-Precision',\n",
    "                       'W-Recall', 'W-F1-Score'])\n",
    "\n",
    "# for KK in list([7,10,18,22,55,77]): \n",
    "KK=22\n",
    "\n",
    "random.seed(KK)\n",
    "np.random.seed(KK)\n",
    "torch.manual_seed(KK)\n",
    "tf.random.set_seed(KK)\n",
    "\n",
    "\n",
    "\n",
    "img_IDs = []\n",
    "updated_pathnames = []\n",
    "dir_path = os.path.join(os.getcwd() , 'TRAINING')\n",
    "img_filenames= natsort.natsorted(os.listdir(dir_path))  \n",
    "\n",
    "# get the ids from the images, where images are having three channels; omit images if channels != 3\n",
    "for i, filename in enumerate(img_filenames):\n",
    "#     print(i, filename)\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        path_name=os.path.join(dir_path, filename)\n",
    "        im = Image.open(path_name)\n",
    "        im = im.resize((224, 224))  # Resize the image to (224, 224)\n",
    "        imnp = np.array(im)\n",
    "        if len(imnp.shape) != 3:\n",
    "            continue\n",
    "        img_IDs.append(filename)\n",
    "        updated_pathnames.append(path_name)  #gives 9986 results\n",
    "\n",
    "\n",
    "def get_common_strings(list1, list2):\n",
    "    return list(set(list1) & set(list2))\n",
    "\n",
    "\n",
    "txt_file = pd.read_csv('./training1.csv', delimiter='\\t')\n",
    "sub_txt_file = txt_file[['file_name', 'misogynous']]\n",
    "sub_txt_file['prefix_file_name'] = sub_txt_file['file_name'].str.extract('(\\d+)').astype(int)\n",
    "sub_txt_file = sub_txt_file.sort_values(by='prefix_file_name', ascending=True)\n",
    "\n",
    "\n",
    "txt_IDs = list(sub_txt_file.file_name)  #from the text file where we have text description \n",
    "common_strings = get_common_strings(txt_IDs, img_IDs)\n",
    "print('len of common strings', len(common_strings))\n",
    "\n",
    "common_IDs = natsort.natsorted(common_strings)\n",
    "print(len(common_IDs), len(txt_IDs), len(img_IDs))\n",
    "\n",
    "\n",
    "filtered_txt_file = sub_txt_file[sub_txt_file['file_name'].isin(common_IDs)].reset_index(drop=True)\n",
    "filtered_txt_file\n",
    "\n",
    "train_labels= list(filtered_txt_file['misogynous'])\n",
    "\n",
    "\n",
    "train_images_path = []\n",
    "for i in common_IDs:\n",
    "    path_name=os.path.join(dir_path, i)\n",
    "    train_images_path.append(path_name)\n",
    "\n",
    "\n",
    "\n",
    "######################################## TEST IMAGES PATH ###################################\n",
    "# Get IDs from test text data\n",
    "test1 = pd.read_csv('Test.csv', delimiter='\\t')\n",
    "test_labels = pd.read_csv('test_labels.txt', \n",
    "                          delimiter='\\t',\n",
    "                         header=None)\n",
    "\n",
    "test_labels.columns = ['file_name', \n",
    "                      \"misogynous\",\n",
    "                       \"shaming\",\n",
    "                       \"stereotype\",\n",
    "                       \"objectification\",\n",
    "                       \"violence\"]\n",
    "\n",
    "merged_test = pd.merge(test1, test_labels, on='file_name', how='inner')\n",
    "\n",
    "# Sort the dataframe with natural ordering of the IDs\n",
    "merged_test['prefix_file_name'] = merged_test['file_name'].str.extract('(\\d+)').astype(int)\n",
    "# Assuming 'df' is your DataFrame\n",
    "merged_test1 = merged_test.sort_values(by='prefix_file_name', ascending=True)\n",
    "merged_test1\n",
    "\n",
    "# # train = train.rename(columns={'Text Transcription': 'text'})\n",
    "test2 = merged_test1.rename(columns={'Text Transcription': 'text'})\n",
    "test_txt_IDs = list(test2['file_name'])\n",
    "\n",
    "\n",
    "\n",
    "# GET IDs from test images data\n",
    "test_dir_path = os.path.join(os.getcwd() , 'TEST')\n",
    "test_img_IDs = natsort.natsorted(os.listdir(test_dir_path))  \n",
    "\n",
    "# Get COmmon IDS\n",
    "test_common_strings = get_common_strings(test_txt_IDs, test_img_IDs)\n",
    "test_common_IDs = natsort.natsorted(test_common_strings)\n",
    "print('len of common strings', len(test_common_IDs))\n",
    "\n",
    "\n",
    "test_images_path = []\n",
    "for i in test_common_IDs:\n",
    "    path_name=os.path.join(test_dir_path, i)\n",
    "    test_images_path.append(path_name)\n",
    "\n",
    "# test_images_path\n",
    "tst_filtered_txt_file = test2[test2['file_name'].isin(test_common_IDs)].reset_index(drop=True)\n",
    "test_labels= list(tst_filtered_txt_file['misogynous'])\n",
    "# test_images_path[-1], test_labels[-1]\n",
    "\n",
    "print(len(test_images_path), len(test_labels))\n",
    "\n",
    "\n",
    "# # total_img_info =train_img_info(train_images_path, train_labels)\n",
    "\n",
    "def img_info(images_path, labels, batch_size=1):\n",
    "    for i in range(0, len(images_path), batch_size):\n",
    "#         image_batch = []\n",
    "        for j in range(i, min(i+batch_size, len(images_path))):\n",
    "            image_path = images_path[j]\n",
    "            image = Image.open(image_path)\n",
    "            image_info = { 'image': image, 'image_file_path': image_path, 'labels': labels[j] }\n",
    "#             image_batch.append(image_info)\n",
    "\n",
    "        yield image_info\n",
    "\n",
    "train_images_path1 = train_images_path[0:3000]\n",
    "train_labels1 = train_labels[0:3000]\n",
    "\n",
    "\n",
    "total_img_info1=[]\n",
    "for k in img_info(train_images_path1, train_labels1, batch_size=1):\n",
    "    total_img_info1.append(k)\n",
    "\n",
    "\n",
    "print('total image info created')    \n",
    "\n",
    "import pickle\n",
    "# Save the list as a pickle object\n",
    "with open('15_total_img_info1.pickle', 'wb') as f:\n",
    "    pickle.dump(total_img_info1, f)\n",
    "\n",
    "print(\"Results saved as pickle object.\")\n",
    "\n",
    "del total_img_info1,  train_images_path1 , train_labels1\n",
    "\n",
    "###########################################################\n",
    "\n",
    "\n",
    "train_images_path2 = train_images_path[3000:6000]\n",
    "train_labels2 = train_labels[3000:6000]\n",
    "\n",
    "\n",
    "total_img_info2=[]\n",
    "for k in img_info(train_images_path2, train_labels2, batch_size=1):\n",
    "    total_img_info2.append(k)\n",
    "\n",
    "\n",
    "\n",
    "print('total image info created')    \n",
    "\n",
    "import pickle\n",
    "# Save the list as a pickle object\n",
    "with open('15_total_img_info2.pickle', 'wb') as f:\n",
    "    pickle.dump(total_img_info2, f)\n",
    "\n",
    "print(\"Results saved as pickle object.\")\n",
    "\n",
    "del total_img_info2, train_images_path2 , train_labels2 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "\n",
    "train_images_path3 = train_images_path[6000:len(train_images_path)]\n",
    "train_labels3 = train_labels[6000:len(train_images_path)]\n",
    "\n",
    "\n",
    "total_img_info3=[]\n",
    "for k in img_info(train_images_path3, train_labels3, batch_size=1):\n",
    "    total_img_info3.append(k)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('total image info created')    \n",
    "\n",
    "import pickle\n",
    "# Save the list as a pickle object\n",
    "with open('15_total_img_info3.pickle', 'wb') as f:\n",
    "    pickle.dump(total_img_info3, f)\n",
    "\n",
    "print(\"Results saved as pickle object.\")\n",
    "\n",
    "\n",
    "del total_img_info3, train_images_path3 , train_labels3 \n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# List of pickle file names you want to load\n",
    "pickle_files = ['15_total_img_info1.pickle', '15_total_img_info2.pickle', '15_total_img_info3.pickle']\n",
    "\n",
    "# Create an empty list to store the combined results\n",
    "total_img_info = []\n",
    "\n",
    "# Load each pickle file and extend the combined_results list\n",
    "for file_name in pickle_files:\n",
    "    fn=os.path.join(os.getcwd(), file_name)\n",
    "    with open(fn, 'rb') as f:\n",
    "        loaded_list = pickle.load(f)\n",
    "        total_img_info.extend(loaded_list)\n",
    "\n",
    "\n",
    "for k in img_info(test_images_path, test_labels, batch_size=1):\n",
    "    total_img_info.append(k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d782cbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10986, 3, 224, 224]) torch.Size([10986])\n",
      "Model Configuration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "/home/nitesh/env/dev38/python38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8986\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4496\n",
      "  Number of trainable parameters = 85800194\n"
     ]
    }
   ],
   "source": [
    "modelpath = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "feature_extractor =  ViTImageProcessor.from_pretrained(modelpath)\n",
    "\n",
    "# Convert the image_info to a tensor dictionary\n",
    "def convert_to_tensor(image_info):\n",
    "    image = image_info['image']\n",
    "    image = F.resize(image, (224, 224))  # Resize the image to the desired shape\n",
    "    pixel_values = feature_extractor(images=image, return_tensors='pt')['pixel_values']\n",
    "    image_info['pixel_values'] = pixel_values.squeeze()  # Remove the batch dimension\n",
    "    return image_info\n",
    "\n",
    "total_image_info_tf = [convert_to_tensor(image_info) for image_info in total_img_info]\n",
    "\n",
    "\n",
    "# Convert data_dict to tensor dataset (torch.utils.data.Dataset)\n",
    "data_dict = {\n",
    "    'pixel_values': torch.stack([image_info['pixel_values'] for image_info in total_image_info_tf]),\n",
    "    'labels': torch.tensor([image_info['labels'] for image_info in total_image_info_tf])\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(data_dict['pixel_values'].shape, data_dict['labels'].shape)\n",
    "\n",
    "\n",
    "labels = ['non_misog', 'misog']\n",
    "\n",
    "print('Model Configuration')\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    modelpath,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")\n",
    "\n",
    "#     print(model)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        self.pixel_values = data_dict['pixel_values']\n",
    "        self.labels = data_dict['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the pixel values and labels for the given index\n",
    "        pixel_values = self.pixel_values[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Return them as tensors\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': label,\n",
    "        }\n",
    "\n",
    "num_val_samples = 1000\n",
    "num_test_samples = 1000\n",
    "\n",
    "# # Calculate the number of samples to use for the validation set (10% of the total samples)\n",
    "# num_val_samples = int(0.1 * len(data_dict['pixel_values']))\n",
    "# num_test_samples = int(0.1 * len(data_dict['pixel_values']))\n",
    "\n",
    "\n",
    "# Split the data_dict into training, validation, and test sets\n",
    "train_data_dict = {\n",
    "    'pixel_values': data_dict['pixel_values'][0:10986-1000-1000],\n",
    "    'labels': data_dict['labels'][0:10986-1000-1000]\n",
    "}\n",
    "\n",
    "\n",
    "val_data_dict = {\n",
    "    'pixel_values': data_dict['pixel_values'][10986-1000-1000:10986-1000],\n",
    "    'labels': data_dict['labels'][10986-1000-1000:10986-1000]\n",
    "}\n",
    "\n",
    "test_data_dict = {\n",
    "    'pixel_values': data_dict['pixel_values'][10986-1000:],\n",
    "    'labels': data_dict['labels'][10986-1000:]\n",
    "}\n",
    "\n",
    "\n",
    "# Create training and validation datasets\n",
    "training_dataset = CustomImageDataset(train_data_dict)\n",
    "val_dataset = CustomImageDataset(val_data_dict)\n",
    "test_dataset = CustomImageDataset(test_data_dict)\n",
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-base-beans-demo-v5\",\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=4,\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=0.0002,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "\n",
    "# DataCollator for batch processing\n",
    "class DataCollator:\n",
    "    def __call__(self, batch):\n",
    "        pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "        labels = torch.tensor([item['labels'] for item in batch])\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "data_collator = DataCollator()\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Predict labels on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "# print(predictions.predictions)\n",
    "\n",
    "# Get the predicted label IDs\n",
    "predicted_label_ids = predictions.predictions.argmax(-1)\n",
    "\n",
    "# Get the label names from the ID to label mapping\n",
    "id2label = {str(i): label for i, label in enumerate(labels)}\n",
    "predicted_labels = [id2label[str(label_id)] for label_id in predicted_label_ids]\n",
    "\n",
    "predicted_labels\n",
    "\n",
    "# Get the ground truth labels\n",
    "true_labels = [id2label[str(label.item())] for label in test_dataset[:]['labels']]\n",
    "true_labels\n",
    "\n",
    "# Generate the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(true_labels, predicted_labels)\n",
    "print(report)\n",
    "\n",
    "\n",
    "def export_classification_report(report, mod, eval_set=None):\n",
    "    lines = report.split('\\n')\n",
    "    data = lines[2:4] + lines[5:9]\n",
    "    data = [line.split() for line in data]\n",
    "\n",
    "    acc = float(data[2][1])\n",
    "    m_prec = float(data[3][2])\n",
    "    m_recall = float(data[3][3])\n",
    "    m_f1 = float(data[3][4])\n",
    "    w_prec = float(data[4][2])\n",
    "    w_recall = float(data[4][3])\n",
    "    w_f1 = float(data[4][4])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Model': [mod],\n",
    "        'EvalSet': [eval_set],\n",
    "        'Accuracy': [acc],\n",
    "        'M-Precision': [m_prec],\n",
    "        'M-Recall': [m_recall],\n",
    "        'M-F1-Score': [m_f1],\n",
    "        'W-Precision': [w_prec],\n",
    "        'W-Recall': [w_recall],\n",
    "        'W-F1-Score': [w_f1],\n",
    "    }) \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "modname = 'Vit_MAMI_Imgs'\n",
    "\n",
    "results_val= export_classification_report(report, modname, eval_set='test')\n",
    "PERFORMANCE_RESULTS = pd.concat([PERFORMANCE_RESULTS, results_val])\n",
    "\n",
    "del train_data_dict, val_data_dict, test_data_dict, total_img_info, total_image_info_tf \n",
    "del filtered_txt_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERFORMANCE_RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc13c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE_RESULTS.to_excel('16_classification_report_Model_ViT_MAMI_Imgs_result3_final.xlsx', \n",
    "#                       index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcf5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
