{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "101c0bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nitesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2023-06-03 14:02:12.627597: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\n",
      "transformer version: 4.15.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import os\n",
    "import pandas as pd\n",
    "import natsort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "sw_nltk = stopwords.words('english')\n",
    "from gensim.models import Word2Vec, word2vec\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, AutoModel, AdamW\n",
    "import torch\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "from PIL import Image\n",
    "import natsort\n",
    "import os\n",
    "\n",
    "## Import/Load Dependencies:\n",
    "# timm==0.4.12\n",
    "# transformers==4.15.0  #using it as replacment of 4.22.0\n",
    "# fairscale==0.4.4\n",
    "# pycocoevalcap\n",
    "\n",
    "if os.getcwd() != \"/home/nitesh/Documents/MY_THESIS/MAMI/BLIP\":\n",
    "    %cd BLIP\n",
    "print(os.getcwd())\n",
    "\n",
    "print('transformer version:', transformers.__version__)\n",
    "# !git clone https://github.com/salesforce/BLIP\n",
    "# Ref: https://github.com/salesforce/BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b677ed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd3e1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./DistilBERT_Caption_EmbModel_&_Tokenizer/DistilBERT_tokenizer/tokenizer_config.json',\n",
       " './DistilBERT_Caption_EmbModel_&_Tokenizer/DistilBERT_tokenizer/special_tokens_map.json',\n",
       " './DistilBERT_Caption_EmbModel_&_Tokenizer/DistilBERT_tokenizer/vocab.txt',\n",
       " './DistilBERT_Caption_EmbModel_&_Tokenizer/DistilBERT_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "\n",
    "model_directory = \"./DistilBERT_Caption_EmbModel_&_Tokenizer/DistilBERT_Model\"\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "tokenizer_directory = \"./DistilBERT_Caption_EmbModel_&_Tokenizer/DistilBERT_tokenizer\"\n",
    "if not os.path.exists(tokenizer_directory):\n",
    "    os.makedirs(tokenizer_directory)\n",
    "    \n",
    "    \n",
    "model.save_pretrained(model_directory)\n",
    "tokenizer.save_pretrained(tokenizer_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1960f74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=64, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "projection_layer = torch.nn.Linear(768, 64)  # Add a linear layer to project embeddings to size 64\n",
    "projection_layer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78050d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \n",
    "train = pd.read_csv('train_BLIP_captions.csv')\n",
    "\n",
    "test = pd.read_csv('test_BLIP_captions.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "168cab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(\n",
    "    train[\"text\"].tolist(),\n",
    "    padding=True,\n",
    "    max_length=20,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test[\"text\"].tolist(),\n",
    "    padding=True,\n",
    "    max_length=20,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_outputs = model(**train_encodings.to(device))\n",
    "    train_embeddings = train_outputs.last_hidden_state[:, 0, :]  # Extract embeddings for the [CLS] token\n",
    "    train_embeddings = projection_layer(train_embeddings)  # Project embeddings to size 65\n",
    "    \n",
    "    test_outputs = model(**test_encodings.to(device))\n",
    "    test_embeddings = test_outputs.last_hidden_state[:, 0, :]  # Extract embeddings for the [CLS] token\n",
    "    test_embeddings = projection_layer(test_embeddings)  # Project embeddings to size 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "664b522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeddings shape: torch.Size([9986, 64])\n",
      "test embeddings shape: torch.Size([1000, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"train embeddings shape:\", train_embeddings.shape)\n",
    "print(\"test embeddings shape:\", test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21acf8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1932,  0.0834, -0.7582,  ..., -0.4121,  0.2424, -0.0134],\n",
       "        [-0.2080,  0.0656, -0.7333,  ..., -0.3051,  0.0820, -0.0117],\n",
       "        [-0.1806,  0.1146, -0.6511,  ..., -0.2794,  0.2367, -0.0113],\n",
       "        ...,\n",
       "        [-0.1549,  0.1752, -0.6844,  ..., -0.3982,  0.2638, -0.0273],\n",
       "        [-0.0293,  0.1082, -0.6396,  ..., -0.4435,  0.1650,  0.0063],\n",
       "        [-0.1633,  0.0984, -0.7528,  ..., -0.3874,  0.1956, -0.0389]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f3afbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1756,  0.0793, -0.7914,  ..., -0.3138,  0.2691, -0.0357],\n",
       "        [-0.2032,  0.0948, -0.5722,  ..., -0.3676,  0.2403, -0.1027],\n",
       "        [-0.2120,  0.1770, -0.6337,  ..., -0.3392,  0.3496, -0.1244],\n",
       "        ...,\n",
       "        [-0.2588,  0.1198, -0.7081,  ..., -0.4302,  0.2636, -0.0382],\n",
       "        [-0.1236,  0.1029, -0.6533,  ..., -0.3789,  0.2802,  0.0214],\n",
       "        [-0.1453,  0.1984, -0.7111,  ..., -0.4090,  0.1909,  0.0190]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe8ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
